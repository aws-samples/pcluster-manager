[{"uri":"https://pcluster.cloud/","title":"AWS ParallelCluster Manager","tags":[],"description":"","content":"\nAWS ParallelCluster Manager is a web UI built for AWS ParallelCluster that makes it easy to create, update, and access HPC clusters. It gives you a quick way to connect to clusters via shell SSM or remote desktop DCV. The UI is built using the AWS ParallelCluster API, making it fully compatible with any cluster 3.X or greater regardless of if you create the cluster through the API, CLI or Web UI.\nWant to request a new feature? First checkout the Roadmap If you don\u0026rsquo;t already see your feature, open a feature request AWS ParallelCluster Manager Architecture AWS ParallelCluster Manager is built on a fully serverless architecture, in most cases it\u0026rsquo;s completely free to run, you just pay for the clusters themselves.\nCosts AWS ParallelCluster Manager is built on a serverless architecture and falls into the free-tier for most uses. I\u0026rsquo;ve detailed the dependency services and their free-tier limits below:\nService Free Tier Cognito 50,000 Monthly Active Users API Gateway 1M Rest API Calls Lambda 1M free requests / month \u0026amp; 400,000 GB-seconds of compute / month Image Builder No-Cost except EC2 EC2 ~15 mins one-time to build Container Image Typical usage will likely cost \u0026lt; $1 / month.\nGet Started You can get started with your first cluster in as little as ~15 minutes following 1 - Setup\n"},{"uri":"https://pcluster.cloud/01-getting-started.html","title":"Setup","tags":[],"description":"","content":"This workshop walks you through setting up Pcluster Manager. You learn how to navigate the AWS Management Console, access relevant services, and how to deploy a basic infrastructure. Specifically, you learn how to:\nSign in to the AWS Management Console and explore it. Deploy Pcluster Manager Connect to the Web UI You can proceed to the next stage of the workshop once the preparation completed and the stacks deployed.\nSome pages are long and you will need to scroll down to read all the instructions.\n"},{"uri":"https://pcluster.cloud/01-getting-started/01-aws-console-login.html","title":"a. Access AWS","tags":[],"description":"","content":"Depending on your workshop, you may access the AWS Management Console through direct sign-in (here) or as directed by your trainer. To sign in, enter your AWS Account ID or alias, IAM user name, and password that was provided to you for this lab.\nAfter you sign in, take a few minutes to explore the navigation components of the AWS Management Console.\nA search bar allows you to quickly locate services based on text. Recently visited services are located below the search bar. In the toolbar, the Services drop-down menu populates a list of all services. The Support drop-down menu includes links to support and documentation. The Region drop-down menu allows you to select a specific AWS Region. Start this workshop by selecting an AWS Region:\nChoose the Region drop-down menu, then choose US East (N. Virginia) us-east-1.\n"},{"uri":"https://pcluster.cloud/01-getting-started/02-pcmanager-install.html","title":"b. Deploy Pcluster Manager","tags":[],"description":"","content":"The AWS ParallelCluster API has been released with the version 3 of AWS ParallelCluster. It enables you to to manage clusters though an API hosted on AWS and build workflows to manage your cluster lifecycle with Python.\nPcluster Manager is a web UI that built upon the AWS ParallelCluster CLI that you can use to manage your compute clusters. It\u0026rsquo;ll take about 20 minutes to deploy both stacks. You will initiate the deployment now to have it ready by the time you need it.\nDeploy the Pcluster Manager stack by clicking on the link in your region. If you don\u0026rsquo;t have a prefered region use Ohio (us-east-2). Region Launch Ohio (us-east-2) Launch Stack (us-east-2) North Virginia (us-east-1) Launch Stack (us-east-1) Ireland (eu-west-1) Launch Stack (eu-west-1) Frankfurt (eu-central-1) Launch Stack (eu-central-1) GovCloud West (us-gov-west-1) Launch Stack (us-gov-west-1) Oregon (us-west-2) Launch Stack (us-west-2) California (us-west-1) Launch Stack (us-west-1) London (eu-west-2) Launch Stack (eu-west-2) Paris (eu-west-3) Launch Stack (eu-west-3) Stockholm (eu-north-1) Launch Stack (eu-north-1) Middle East (me-south-1) Launch Stack (me-south-1) South America (sa-east-1) Launch Stack (sa-east-1) Canada (ca-central-1) Launch Stack (ca-central-1) Hong Kong (ap-east-1) Launch Stack (ap-east-1) Tokyo (ap-northeast-1) Launch Stack (ap-northeast-1) Seoul (ap-northeast-2) Launch Stack (ap-northeast-2) Mumbai (ap-south-1) Launch Stack (ap-south-1) Singapore (ap-southeast-1) Launch Stack (ap-southeast-1) Sydney (ap-southeast-2) Launch Stack (ap-southeast-2) The AWS Console opens on the AWS CloudFormation panel to deploy your stack. Update the field AdminUserEmail with a valid email to receive a temporary password in order to connect to the Pcluster Manager GUI. Leave the other fields with their default values and click Next to proceed to Step 3. Scroll down to the bottom of the Stage 3 page (Configure stack options) and click Next. Scroll down to the bottom of the Stage 4 page (Review) and click on the the two tick boxes to create new IAM resources. Once done, click on Create stack. The stack is deploying using AWS CloudFormation. It will take ~20 minutes to deploy the AWS ParallelCluster API and Pcluster Manager GUI. In the meantime, you will complete the first part of the lab. Continue to the next page to define the configuration of your first HPC system in AWS.\nEnsure that you entered a valid email when deploying the Pcluster Manager stack. This email will be used to send you the credentials to connect to Pcluster Manager. If the email you will have to delete and recreate it which may delay your progression.\n"},{"uri":"https://pcluster.cloud/01-getting-started/03-pcmanager-connect.html","title":"c. Connect to Pcluster Manager","tags":[],"description":"","content":" During deployment you received an email titled [PclusterManager] Welcome to Pcluster Manager, please verify your account.. Click on the link to login with the temporary code provided. Enter the credentials using the email you used when deploying the stack and the temporary password from the email above. You will be asked to provide a new password. Enter a new password to complete signup. Congrats! You are ready to create your HPC cluster in AWS. Let\u0026rsquo;s do that in the next section.\nTo get the URL outside of the email, go to AWS CloudFormation \u0026gt; pcluster-manager \u0026gt; Outputs then click on the PclusterManagerUrl to connect. "},{"uri":"https://pcluster.cloud/01-getting-started/04-summary.html","title":"d. Summary","tags":[],"description":"","content":"In this part of the lab you deployed the AWS ParallelCluster API and Pcluster Manager. You should see a screen that looks like:\n"},{"uri":"https://pcluster.cloud/02-tutorials.html","title":"Tutorials","tags":[],"description":"","content":" Tutorial Description 📱 SMS Multi-Factor Authentication Setup Multi-Factor Authentication for user login. 🪄 Slurm Accounting Track job duration, cost, instance type by user. 💾 Memory Scheduling Schedule using the --mem slurm flag. 💰 Cost Tags Track job costs in AWS Cost Explorer by user and project. ⇓ Downloading Download files at cluster start. "},{"uri":"https://pcluster.cloud/02-tutorials/01-setup-sms.html","title":"a. Setup SMS for MFA 📱","tags":[],"description":"","content":"To enable Multi-Factor Authentication (MFA) with Pcluster Manager there\u0026rsquo;s two setup steps that need to be completed.\nSetup an Origination number Add a sandbox number 1. Setup an Origination Number Navigate to Pinpoint Phone Numbers Console \u0026gt; Click Request Phone Number\nFill out the following options:\nOption Description Country [Your Country] Number Type Toll Free Capabilities SMS Default Message Type Transactional Click Next \u0026gt; Request\n2. Add a sandbox number Next navigate to the SNS SMS Console \u0026gt; Click Add a phone number\nEnter the phone number of the user and set verification message language\nYou\u0026rsquo;ll receive a text message, enter that code to verify your number\n3. Test with Pcluster Manager Now that you\u0026rsquo;ve gotten the SMS portion setup you can go ahead and login. You\u0026rsquo;ll see a screen after you enter your username/password that looks like:\nIf everthing is setup properly you\u0026rsquo;ll receive a text message that allows you to login.\n"},{"uri":"https://pcluster.cloud/02-tutorials/02-slurm-accounting.html","title":"b. Slurm Accounting 🪄","tags":[],"description":"","content":"In this tutorial we will work through setting up Slurm Accounting. This enables many features within slurm, including job resource tracking and providing a necessary building block to slurm federation.\nStep 1 - Setup External Accounting Database The first requirement is to setup an external database that Slurm can use to store the accounting data.\nUse the following CloudFormation Quick-Create link to create the database in your AWS account. Note that if you would like to create the database in a different region, change the value of the region parameter in the URL to the region of your choice and reload the page.\nDeploy Accounting Database When you\u0026rsquo;re creating the stack, be sure to specify the VPC ID and Subnets parameters to correspond to the VPC where you are creating the stack. All other values should be suitable as defaults, however feel free to change the database instance type depending on your workload needs.\nChange the region in the URL to create the stack in a region separate from us-east-1.\nStep 2- Retrieve the outputs from the CloudFormation stack Once the stack has reached a Completed state. You will need to go to the Outputs tab of the stack and make note of the properties as they will be used in the creation of your cluster.\nStep 3 - Add permissions to your lambda In order to allow our cluster access to secrets we need to add an additional IAM policy.\nGo to the Lambda Console (deeplink) and search for ParallelClusterFunction Select the function then Configuration \u0026gt; Permissions \u0026gt; Click on the role under Role name. Select the AWSXRayDaemonWriteAccess policy and remove it Select Add permissions \u0026gt; Attach policies Search for AdministratorAccess \u0026gt; click Attach policies Step 4 - Create Your Cluster Next, go to Pcluster Manager and choose the option to create a new cluster.\nSelect Wizard option and click next Cluster Properties Choose a suitable name for your cluster, and then in the Cluster Properties window, be sure to choose the VPC that you used when creating the slurm-accounting CloudFormation stack.\nHeadNode Properties You will need to enable the Virtual Console option as that allows Pcluster Manager to interact with the cluster directly:\nBe sure to also enable the Security Group referenced in the CloudFormation outputs so that the HeadNode can access the database.\nNext we\u0026rsquo;ll enable a known script that will install slurm accounting on the HeadNode.\nChoose the advanced options Under the On Configured option, Choose the Multi-Script Runner which has some pre-programmed scripts in it In the search box choose Slurm Accounting Fill in the values for the Secret ARN and RDS Endpoint from the CloudFormation output Under IAM Policies add arn:aws:iam::aws:policy/SecretsManagerReadWrite so that the HeadNode can access the password to the database. Be sure to actually click Add so that it is added to the list. Review Config After you\u0026rsquo;ve configured the HeadNode, Filesystem and Queues, you\u0026rsquo;ll be asked to review the config. The following parameters must be set:\nParameter Description AdditionalSecurityGroups SlurmDbSecurityGroupId (CloudFormation) AdditionalIamPolicies AmazonSSMManagedInstanceCore, SecretsManagerReadWrite (CloudFormation) CustomActions/OnNodeConfigured multi-runner.py Arg 0: Accounting Script slurm-accounting.sh Arg 1: SECRET_ARN SlurmDbPasswordSecretArn (CloudFormation) Arg 2: RDS Endpoint SlurmDbEndpoint (CloudFormation) Arg 3: Port Default is 3306 Here\u0026rsquo;s an example config file to reference, take a look a the comments to see what\u0026rsquo;s required:\nHeadNode: InstanceType: t2.micro Networking: SubnetId: subnet-12345678910 AdditionalSecurityGroups: - sg-12345678910 # Security Group `SlurmDbSecurityGroup` Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy: arn:aws:iam::aws:policy/SecretsManagerReadWrite # Policy `SecretsManagerReadWrite` CustomActions: OnNodeConfigured: Script: \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/multi-runner.py Args: - \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/slurm-accounting.sh - \u0026#39;-arn:aws:secretsmanager:us-east-2:1234567890:secret:DbPasswdSecret\u0026#39; # `SlurmDbPasswordSecretArn` - \u0026#39;-slurmdb-rds-instance.c123456.us-east-2.rds.amazonaws.com\u0026#39; # RDS Endpoint `SlurmDbEndpoint` - \u0026#39;-3306\u0026#39; # Default Port 3306 Scheduling: Scheduler: slurm SlurmQueues: - Name: queue0 ComputeResources: - Name: queue0-t2-micro MinCount: 0 MaxCount: 4 InstanceType: t2.micro Networking: SubnetIds: - subnet-12345678910 Region: us-east-2 Image: Os: alinux2 Step 5 - Submit a job Once the cluster has been successfully created, go to the Scheduling tab and select Submit Job\nChoose a name for your job, a number of nodes to run under, choose to Run a command and provide a simple sleep 30 command.\nStep 6 - View the Accounting Tab Once you\u0026rsquo;ve submitted a job, you can see the job information under the Accounting tab\nYou can use any of the filters at the top to narrow down the number of jobs in the view to select specific jobs.\nIf you choose the Job ID in the left column you can see further detials about the job.\n"},{"uri":"https://pcluster.cloud/02-tutorials/03-memory-scheduling.html","title":"c. Memory Scheduling 💾","tags":[],"description":"","content":" This feature was launched in AWS ParallelCluster 3.2.0 and can be enabled by enabling Slurm Memory Based Scheduling Enabled in the HeadNode configuration screen. See Slurm memory-based scheduling in the AWS ParallelCluster docs for more info.\nSlurm supports memory based scheduling via a --mem or --mem-per-cpu flag provided at job submission time. This allows scheduling of jobs with high memory requirements, allowing users to guarantee a set amount of memory per-job or per-process.\nFor example users can run:\nsbatch --mem-per-cpu=64G -n 8 ... To get 8 vcpus and 64 gigs of memory.\nIn order to add in memory information, we have a managed post-install script that can be setup with Pcluster Manager. This script sets the RealMemory to 85% of the available system memory, allowing 15% to system processes.\nSetup with 3.2.0 When setting up a cluster with version \u0026gt; 3.2.0, simply toggle Slurm Memory Based Scheduling Enabled to on:\nOptionally you can setup the specific amount of memory that Slurm configures on each node, however I don\u0026rsquo;t reccomend doing this as it may results in a job over-allocating memory.\nSetup with \u0026lt; 3.2.0 To enable this in versions \u0026lt; 3.2.0, create a new cluster and in the HeadNode configuration screen, click on the \u0026ldquo;Advanced\u0026rdquo; dropdown and add in the managed Memory script:\nThen add the following managed IAM policy to the head node:\narn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess On the last screen your config should look similar to the following, note you\u0026rsquo;ll minimally need AmazonEC2ReadOnlyAccess and https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/mem.sh script.\nHeadNode: InstanceType: c5a.xlarge Ssh: KeyName: keypair Networking: SubnetId: subnet-123456789 Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy: arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess CustomActions: OnNodeConfigured: Script: \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/multi-runner.py Args: - \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/mem.sh Scheduling: Scheduler: slurm SlurmQueues: - Name: cpu ComputeResources: - Name: cpu-hpc6a48xlarge MinCount: 0 MaxCount: 100 InstanceType: hpc6a.48xlarge Efa: Enabled: true Networking: SubnetIds: - subnet-123456789 PlacementGroup: Enabled: true Region: us-east-2 Image: Os: alinux2 Test When the cluster has been created you can check the memory settings for each instance:\n$ scontrol show nodes | grep RealMemory NodeName=cpu-dy-cpu-hpc6a48xlarge-1 CoresPerSocket=1 ... RealMemory=334233 AllocMem=0 FreeMem=N/A Sockets=96 Boards=1 ... You\u0026rsquo;ll see that for the hpc6a.48xlarge instance, which has 384 GB of memory that RealMemory=334233 or 384 GB * .85 = 334.2 GB.\nTo schedule a job with memory constraints you can use the --mem flag. See the Slurm sbatch docs for more info.\n$ salloc --mem 8GB You can see the requested memory for that job by running:\nsqueue -o \u0026#34;%.18i %.9P %.8j %.8u %.2t %.10M %.6D %.5m %.5c %R\u0026#34; JOBID PARTITION NAME USER ST TIME NODES MIN_M MIN_C NODELIST(REASON) 3 cpu interact ec2-user R 12:25 1 8G 1 cpu-dy-cpu-hpc6a48xlarge-1 "},{"uri":"https://pcluster.cloud/02-tutorials/04-cost-tracking.html","title":"d. Cost Tracking 💰","tags":[],"description":"","content":"Cost Tracking is essential to every single HPC workload, it\u0026rsquo;s important to be able to track costs per-user, per-project, per-slurm-partition and track these costs overtime.\nThere\u0026rsquo;s two different ways to setup cost tracking, each with their own tradeoffs:\nTo see historical spend on a per-job basis, setup Slurm Accounting 🪄. This will allow you to query for job runtimes, who submitted what jobs, how many instances and duration of the job. This won\u0026rsquo;t show shared resources such as Filesystems or idle time on compute instances. To track costs per-cluster, you can tag ec2 instances, this allows you to generate reports in AWS Cost Explorer breaking down by what you tagged. We\u0026rsquo;re going to setup the requisite scripts to tag the instances, allowing users to submit jobs like:\nsbatch --comment ProjectA And administrators to generate reports like:\nStep 1 - create IAM Policy In order to allow the EC2 instances to modify tags, we need to create an IAM policy that allows tagging.\nGo to the IAM Console \u0026gt; Create Policy Click on json tab and paste in the following: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;budgets:ViewBudget\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:budgets::*:budget/*\u0026#34; } ] } Click next a few times until you\u0026rsquo;re at the \u0026ldquo;Review policy\u0026rdquo; screen. Name it pclusterTagsAndBudget Step 2 - configure HeadNode In Pcluster Manager, when you create the cluster, on the HeadNode section drop down the \u0026ldquo;Advanced Options\u0026rdquo;.\nTurn on Multi-runner script Click \u0026ldquo;Add Script\u0026rdquo; Select the \u0026ldquo;Cost Tags\u0026rdquo; managed script Paste in the arn of the pclusterTagsAndBudget you created above. Step 3 - configure ComputeFleet On the ComputeFleet section drop down the \u0026ldquo;Advanced Options\u0026rdquo;.\nTurn on Multi-runner script Click \u0026ldquo;Add Script\u0026rdquo; Select the \u0026ldquo;Cost Tags\u0026rdquo; managed script Paste in the arn of the pclusterTagsAndBudget you created above. Step 4 - review Next, review your config. It should look similar to the following:\nImage: Os: alinux2 HeadNode: InstanceType: c5.2xlarge Networking: SubnetId: subnet-1234567 Ssh: KeyName: keypair CustomActions: OnNodeConfigured: Script: \u0026gt;- https://raw.githubusercontent.com/sean-smith/pcluster-manager/cost-explorer/resources/scripts/cost-tags.sh Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::1234567890:policy/pclusterTagsAndBudget - Policy: arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore Dcv: Enabled: true Scheduling: Scheduler: slurm SlurmQueues: - Name: cpu Networking: SubnetIds: - subnet-1234567 PlacementGroup: Enabled: true ComputeResources: - Name: cpu-hpc6a48xlarge InstanceType: hpc6a.48xlarge MinCount: 0 MaxCount: 100 Efa: Enabled: true CustomActions: OnNodeConfigured: Script: \u0026gt;- https://raw.githubusercontent.com/sean-smith/pcluster-manager/cost-explorer/resources/scripts/cost-tags.sh Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::822857487308:policy/pclusterTagsAndBudget Tags: - Key: aws-parallelcluster-username Value: NA - Key: aws-parallelcluster-jobid Value: NA - Key: aws-parallelcluster-project Value: NA Region: us-east-2 Step 5 - Submit Job with Project To create a project, edit the file /opt/slurm/etc/projects_list.conf:\nec2-user=ProjectA, ProjectB userA=ProjectA, ProjectC In this file you\u0026rsquo;ll find a list of users and the projects associated with them. When you submit a job it\u0026rsquo;ll require you to select a project from that file:\n$ sbatch submit.sh You need to specify a project. \u0026#34;--comment ProjectName\u0026#34; $ sbatch --comment ProjectB submit.sh Submitted batch job 5017 You\u0026rsquo;ll see the following tags get added to the job:\nTag Description aws-parallelcluster-username user who submitted the job aws-parallelcluster-project project name specified in --comment \u0026lt;project-name\u0026gt; aws-parallelcluster-jobid the id of the submitted job parallelcluster:queue-name The Slurm partition these jobs were submitted too. Step 5 - Create a Budget Budgets allow us to track specific project cost over time and get alerted if we\u0026rsquo;re about to hit a cap.\nNavigate to the AWS Budgets Portal \u0026gt; Create Budget Create a Cost Budget Enter in the amount of money and frequency you desire. Select \u0026ldquo;Filter by specific AWS cost dimensions\u0026rdquo; and set the following: Budget Item Description Dimension Tag Tag aws-parallelcluster-project Project Name Name of the project, corresponding to /opt/slurm/etc/projects_list.conf Enter a threshold and email address to get notified when approaching the budget Modify the file /opt/slurm/bin/sbatch on the cluster and set budget=\u0026quot;yes\u0026quot;. #enable or disable the budget checks for the projects budget=\u0026#34;yes\u0026#34; This will check the budget with the same name of the project before the user submits a job and make sure the budget hasn\u0026rsquo;t been exceeded. For example, if ProjectA\u0026rsquo;s budget has been exceeded you\u0026rsquo;ll see:\nsbatch -N 100 --comment ProjectA submit.sh The Project ProjectA does not have more budget allocated for this month. "},{"uri":"https://pcluster.cloud/02-tutorials/05-cloud9.html","title":"e. Cloud9 ☁️","tags":[],"description":"","content":"Cloud9 is a web-based code editor, terminal and file-browser. You can connect to the cluster via Cloud9 and advantage of the following convenient features:\nTerminal Drag \u0026amp; Drop file upload Code editor Setup To setup the connection, we need to run a script to install the Cloud9 packages on the HeadNode.\nFrom the Pcluster Manager GUI create a new cluster. Select the Wizard option: On the HeadNode configuration tab Expand Advanced Options \u0026gt; turn on Multi-Script Runner \u0026gt; select Cloud9 On the final screen review and make sure your config looks similar to the following. The only required parameter is the cloud9.sh script, we also use the SSMManagedInstanceCore policy to connect to the headnode. HeadNode: InstanceType: t2.micro Ssh: KeyName: keypair Networking: SubnetId: subnet-123456789 CustomActions: OnNodeConfigured: Script: \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/multi-runner.py Args: - \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/cloud9.sh Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore Scheduling: Scheduler: slurm SlurmQueues: - Name: queue0 ComputeResources: - Name: queue0-t2-micro MinCount: 0 MaxCount: 4 InstanceType: t2.micro Networking: SubnetIds: - subnet-123456789 Region: us-east-2 Image: Os: alinux2 Once the cluster is CREATE_COMPLETE, copy the public IP address of the HeadNode: Next, navigate to the Cloud9 Console \u0026gt; Create Environment \u0026gt; Give it the same name as the cluster and the following description: Name: Cluster Name Description: AWS ParallelCluster HeadNode Copy the public key and then connect to the cluster using SSM, on the headnode paste in the key at the bottom of the ~/.ssh/authorized_keys file.\nInstall Node.js by executing the following commands on the HeadNode\nsudo yum install -y gcc-c++ make curl -sL https://rpm.nodesource.com/setup_14.x | sudo -E bash - sudo yum install -y nodejs Click Next then review and confirm. If everything worked you should a screen like the following: "},{"uri":"https://pcluster.cloud/02-tutorials/06-downloading.html","title":"f. Downloading ⇓","tags":[],"description":"","content":"This tutorial shows you how you can download a file from an external source at cluster start.\nSetup In order to download a file when you start your cluster,\nFrom the Pcluster Manager GUI create a new cluster. Select the Wizard option: On the HeadNode configuration tab Expand Advanced Options \u0026gt; turn on Multi-Script Runner \u0026gt; select Downloader Note: You can specify either an http://, https:// or s3:// endpoint, however for any s3 location it must reside in the same region as the cluster.\nNote: You may specify any number of additional arguments to the script and it will download each of them to the destination directory.\nOn the final screen review and make sure your config looks similar to the following. The only required parameter is the downloader.sh script. HeadNode: InstanceType: t2.micro Ssh: KeyName: keypair Networking: SubnetId: subnet-123456789 CustomActions: OnNodeConfigured: Script: \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/multi-runner.py Args: - \u0026gt;- https://raw.githubusercontent.com/aws-samples/pcluster-manager/main/resources/scripts/downloader.sh - \u0026#39;-/tmp\u0026#39; - \u0026#39;-https://aws.amazon.com\u0026#39; - \u0026#39;-s3://mybucket/myfile\u0026#39; Scheduling: Scheduler: slurm SlurmQueues: - Name: queue0 ComputeResources: - Name: queue0-t2-micro MinCount: 0 MaxCount: 4 InstanceType: t2.micro Networking: SubnetIds: - subnet-123456789 Region: us-east-2 Image: Os: alinux2 Create your cluster, when your cluster is created each of the files will be downloaded into the destination directory. "},{"uri":"https://pcluster.cloud/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://pcluster.cloud/tags.html","title":"Tags","tags":[],"description":"","content":""}]